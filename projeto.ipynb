{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto para a Disciplina de Inteligência Artificial - 2024.2\n",
    "\n",
    "#### Ludmila Vinólia Guimarães Gomes 20210025065\n",
    "\n",
    "Projeto: Tutor Inteligente\n",
    "\n",
    " - Construir um tutor inteligente usando LLM e engenharia de prompt;\n",
    " - Este tutor irá servir para auxiliar estudantes para aprender um conteúdo, responder perguntas, explicar esses conteúdos e se adaptar ao nível de compreensão do aluno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install groq\n",
    "# !pip install python-dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Teste - código dos slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# carrega variáveis de ambiente do .env\n",
    "load_dotenv()\n",
    "# salva chave para api\n",
    "api_key = os.getenv('API_KEY')\n",
    "# acesso a partir de key\n",
    "client = Groq(api_key=api_key)\n",
    "\n",
    "prompt = input()\n",
    "\n",
    "completion = client.chat.completions.create(\n",
    "  model='deepseek-r1-distill-llama-70b',\n",
    "  messages=[\n",
    "    {\n",
    "      'role': 'user',\n",
    "      'content': prompt\n",
    "    }\n",
    "  ],\n",
    "  temperature=1,\n",
    "  max_tokens=1024,\n",
    "  top_p=0.5,\n",
    "  stream=True,\n",
    "  stop=None\n",
    ")\n",
    "\n",
    "for chunk in completion:\n",
    "  print(chunk.choices[0].delta.content or '', end='')\n",
    "\n",
    "def explain_concept(concept, level='iniciante'):\n",
    "  prompt = 'Explique o conceito de ' + concept + ' em álgebra para um estudante de nível ' + level + '.'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='deepseek-r1-distill-llama-70b',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=2048, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "def generate_problem(concept, difficulty):\n",
    "  prompt = 'Gere um problema envolvendo o conceito de ' + concept + ' de dificuldade ' + difficulty + ' para o estudante resolver. \\\n",
    "    Apenas apresente o problema, não resolva ele.'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='deepseek-r1-distill-llama-70b',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=2048, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "def solve_problem(problem):\n",
    "  prompt = 'Resolva o seguinte problema de álgebra mostrando uma explicação passo-a-passo: ' + problem\n",
    "  completion = client.chat.completions.create(\n",
    "    model='deepseek-r1-distill-llama-70b',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=2048, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "def adjust_explanation(student_answer, correct_answer):\n",
    "  prompt = 'A resposta do estudante foi: ' + student_answer + '. A resposta correta eh: ' + correct_answer + '.\\\n",
    "    Retorne um feedback em português sobre a resposta do estudante com uma explicação adicional caso seja necessário.'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='deepseek-r1-distill-llama-70b',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=2048, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "def tutoring_session(concept, difficulty='fácil'):\n",
    "  # explicar o conceito\n",
    "  print('Explicação:')\n",
    "  explanation = explain_concept(concept, difficulty)\n",
    "  print(explanation)\n",
    "\n",
    "  # gerar um problema para prática\n",
    "  print('\\nProblema para prática:')\n",
    "  problem = generate_problem(concept, difficulty)\n",
    "  print(problem)\n",
    "\n",
    "  # receber resposta do estudante\n",
    "  student_answer = input('\\nSua resposta: ')\n",
    "\n",
    "  # solução correta\n",
    "  print('\\nSolução correta:')\n",
    "  correct_sol = solve_problem(problem)\n",
    "  print(correct_sol)\n",
    "\n",
    "  # ajustar explicação baseado na resposta do estudante\n",
    "  print('\\nFeedback:')\n",
    "  feedback = adjust_explanation(student_answer, correct_sol)\n",
    "  print(feedback)\n",
    "\n",
    "tutoring_session('Solucionar equações lineares', 'intermediário')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projeto - Versão final Tutor Inteligente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "import os\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carrega variável de ambiente com key para API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# carrega variáveis de ambiente do .env\n",
    "load_dotenv()\n",
    "# salva chave para api\n",
    "api_key = os.getenv('API_KEY')\n",
    "# acesso a partir de key\n",
    "client = Groq(api_key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções de reconhecimento para indicar fluxo do programa\n",
    "Reconhece se estudante quer realizar um questionário e terminar programa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# FUNÇÕES PARA RECONHECER COMO PROSSEGUIR NO PROGRAMA\n",
    "\n",
    "# modelo reconhece se o aluno quer fazer questões; retorna 'SIM' ou 'NÃO'\n",
    "def aluno_quer_questionario(prompt_estudante):\n",
    "  prompt = 'Na mensagem entre colchetes [' + prompt_estudante  + '] retorne apenas \"SIM\" se o usuário deseja que você gere perguntas para ele responder \\\n",
    "            e retorne \"NÃO\" se o usuário deseja que você gere perguntas para ele responder.'\n",
    "  # prompt = 'Reconheça explicitamente se o aluno quer gerar um questionário e RESPONDA APENAS com \"NÃO\" \\\n",
    "  #           se o aluno não quer que seja gerado um questionário e com \"SIM\" se ele quiser que seja gerado um questionário. \\\n",
    "  #           A mensagem do aluno está entre colchetes: [' + prompt_estudante + '].'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='llama-3.3-70b-specdec',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=5000, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# modelo reconhece se o aluno quer finalizar programa; retorna 'SIM' ou 'NÃO'\n",
    "def aluno_quer_terminar_programa(prompt_estudante):\n",
    "  # prompt = 'Na mensagem entre colchetes [' + prompt_estudante  + '] retorne apenas \"SIM\" se o usuário deseja finalizar o programa \\\n",
    "  #           e retorne \"NÃO\" se o usuário deseja continuar perguntando algo.'\n",
    "  prompt = 'Reconheça explicitamente e implicitamente e RESPONDA APENAS com \"NÃO\" se o aluno quer continuar conversando com você \\\n",
    "            e com \"SIM\" se ele quiser finalizar o programa. A mensagem do aluno está entre colchetes: [' + prompt_estudante + '].'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='llama-3.3-70b-specdec',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=5000, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para análise do prompt\n",
    "Reconhecendo o conteúdo a ser estudado e o nível de entendimento do aluno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# FUNÇÕES PARA RETIRAR INFORMAÇÕES DO PROMPT DO ALUNO\n",
    "\n",
    "# a partir do prompt do usuário, o modelo reconhece qual o assunto e matéria a ser estudado; retorna string contendo apenas o conteúdo de estudo\n",
    "def reconhece_assunto(prompt_estudante):\n",
    "  prompt = 'Apenas indique o conteúdo de estudo ao qual a mensagem entre colchetes se refere [' + prompt_estudante + ']\\\n",
    "            e não explique nada, diga apenas qual o assunto.'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='llama-3.3-70b-specdec',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=0, max_tokens=2024, top_p=0.4, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# a partir do prompt do usuário, o modelo reconhece o nível do estudante na matéria; retorna nível do aluno\n",
    "def reconhece_nivel_aluno_texto(prompt_estudante):\n",
    "  prompt = 'Indique qual o nível do aluno a partir das suas respostas a partir da mensagem entre colchetes [' + prompt_estudante + ']. \\\n",
    "            Responda simplesmente com o nível do aluno. \\\n",
    "            Os níveis para classificação dos alunos: iniciante, iniciante-intermediário, intermediário, intermediário-avançado, avançado.'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='llama-3.3-70b-specdec',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=5000, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# a partir do prompt do usuário, o modelo reconhece o nível do estudante na matéria; retorna nível do aluno\n",
    "def reconhece_nivel_aluno_questoes(respostas_certas, respostas_aluno):\n",
    "  prompt = 'Compare as respostas certas com as respostas do aluno. Indique qual o nível do aluno a partir das suas respostas. \\\n",
    "            Os níveis para classificação dos alunos: iniciante, iniciante-intermediário, intermediário, intermediário-avançado, avançado. \\\n",
    "            Responda simplesmente com o nível do aluno. \\\n",
    "            As respostas certas estão entre colchetes [' + respostas_certas + '] e as respostas do aluno estão entre chaves {' + respostas_aluno + '}.'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='llama-3.3-70b-specdec',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=5000, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# a partir de todos os prompts do usuário, o modelo reconhece o nível do estudante na matéria e sua evolução; retorna nível e evolução do aluno\n",
    "def reconhece_nivel_prompts_aluno(todos_prompts_aluno):\n",
    "  prompt = 'Reconheça o avanço do aluno ao longo das mensagens dele. Reconheça o avanço dele para cada diferente assunto. Cada mensagem do aluno está separada pelo símbolo \"}\" \\\n",
    "            e todas as mensagens do aluno estão entre colchetes [' + todos_prompts_aluno + ']. Descreva breve e resumidamente a evolução dele e o parabenize!'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='llama-3.3-70b-specdec',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=5000, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções para modelo trabalhar com questionários para o aluno\n",
    "Geração de questões e respostas para as questões feitas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# FUNÇÕES PARA GERAÇÃO E RESOLUÇÃO DE QUESTÕES\n",
    "\n",
    "# o modelo vai gerar questões sobre o assunto reconhecido; retorna as questões geradas\n",
    "def gera_questionario(assunto):\n",
    "  prompt = 'Gere um questionário de 3 questões sobre o assunto ' + assunto\n",
    "  completion = client.chat.completions.create(\n",
    "    model='llama-3.3-70b-specdec',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=5000, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# o modelo vai responder às questões que ele gerou para fornecer respostas corretas para comparação posteriormente; retorna respostas às questões\n",
    "def resolve_questionario(questoes):\n",
    "  prompt = 'Resolva as questões entre colchetes [' + questoes + '].'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='llama-3.3-70b-specdec',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=5000, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções onde o modelo explica o conteúdo e fornece feedbacks para o estudante\n",
    "Explica conteúdo e retorna feedback sobre respostas do aluno às questões geradas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# FUNÇÕES QUE TRAZEM EXPLICAÇÕES E FEEDBACKS PARA O USUÁRIO\n",
    "\n",
    "# a partir do assunto reconhecido e do prompt do usuário, retorna uma explicação sobre o conteúdo passado\n",
    "def explica(assunto, prompt_estudante, mensagens_anteriores_usuario, mensagens_anteriores_modelo):\n",
    "  nivel_aluno = reconhece_nivel_aluno_texto(prompt_estudante)\n",
    "\n",
    "  prompt = 'Responda detalhadamente à mensagem entre colchetes [' + prompt_estudante + '] como um tutor, explicando o conteúdo ' + assunto + ' para um aluno de nível ' + nivel_aluno + '. \\\n",
    "            Se houver referências a mensagens anteriores, use este histórico: \\\n",
    "            Mensagens do aluno: ' + mensagens_anteriores_usuario + ' \\\n",
    "            Suas respostas: ' + mensagens_anteriores_modelo + '. \\\n",
    "            Não mencione o nível do aluno nem o reconhecimento do assunto, apenas use as mensagens como referência.'\n",
    "\n",
    "  completion = client.chat.completions.create(\n",
    "    model='llama-3.3-70b-specdec',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=5000, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content\n",
    "\n",
    "# compara as respostas corretas e as respostas do aluno e envia um feedback para ele; retorna o feedback para o aluno\n",
    "def compara_respostas_feedback(respostas_certas, respostas_aluno):\n",
    "  nivel_aluno = reconhece_nivel_aluno_questoes(respostas_certas, respostas_aluno)\n",
    "\n",
    "  prompt = 'Compare as respostas certas com as respostas do aluno. Explique detalhadamente o que o aluno errou num nível ' + nivel_aluno + 'para que ele entenda. . \\\n",
    "            As respostas certas estão entre colchetes [' + respostas_certas + '] e as respostas do aluno estão entre chaves {' + respostas_aluno + '}.'\n",
    "  completion = client.chat.completions.create(\n",
    "    model='llama-3.3-70b-specdec',\n",
    "    messages=[\n",
    "      {\n",
    "        'role': 'user',\n",
    "        'content': prompt\n",
    "      }\n",
    "    ],\n",
    "    temperature=1, max_tokens=5000, top_p=0.5, stream=False, stop=None\n",
    "  )\n",
    "  return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop de funcionamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================\n",
    "# LOOP DE FUNCIONAMENTO DO CHAT\n",
    "\n",
    "lista_prompts_aluno = []\n",
    "mensagens_anteriores_usuario = []\n",
    "mensagens_anteriores_modelo = []\n",
    "\n",
    "contador = 0\n",
    "\n",
    "while(True):\n",
    "  prompt_estudante = input('Escreva sua dúvida: ')\n",
    "  \n",
    "  # verifica o assunto apenas uma vez no primeiro prompt do aluno\n",
    "  if contador == 0:\n",
    "    assunto = reconhece_assunto(prompt_estudante)\n",
    "    contador += 1\n",
    "\n",
    "  lista_prompts_aluno.append(prompt_estudante)\n",
    "  mensagens_anteriores_usuario.append(prompt_estudante)\n",
    "\n",
    "  if(aluno_quer_questionario(prompt_estudante) == 'SIM'):\n",
    "    questoes = gera_questionario(assunto)\n",
    "    print('Para praticar, geramos as seguintes questões. Tente resolvé-las e ter um feedback de desempenho!\\n\\n\\=================\\n\\nQuestionário:\\n', questoes)\n",
    "    print('\\n\\=================\\n')\n",
    "    respostas_aluno = input('Digite suas respostas: ')\n",
    "    \n",
    "    questoes_resolvidas = resolve_questionario(questoes)\n",
    "    comparacao = compara_respostas_feedback(questoes_resolvidas, respostas_aluno)\n",
    "    print(comparacao)\n",
    "    print('\\n\\=================\\n')\n",
    "\n",
    "    lista_prompts_aluno.append(respostas_aluno)\n",
    "    mensagens_anteriores_usuario.append(respostas_aluno)\n",
    "    mensagens_anteriores_modelo.append(questoes)\n",
    "    mensagens_anteriores_modelo.append(questoes_resolvidas)\n",
    "    mensagens_anteriores_modelo.append(comparacao)\n",
    "  else:\n",
    "    if(aluno_quer_terminar_programa(prompt_estudante) == 'SIM'):\n",
    "      break\n",
    "    explicacao = explica(assunto, prompt_estudante, ('}' + '}'.join(mensagens_anteriores_usuario)), ('}' + '}'.join(mensagens_anteriores_modelo)))\n",
    "    print(explicacao)\n",
    "    print('\\n\\=================\\n')\n",
    "\n",
    "    mensagens_anteriores_modelo.append(explicacao)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salva prompts do usuário de todas as sessões em arquivo\n",
    "Escrita e leitura dos prompts no arquivo para análise de desempenho e evolução do aluno ao longo das sessões"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# salva os prompts do aluno no arquivo\n",
    "f = open(\"prompts_aluno.txt\", \"a\")\n",
    "f.write('}' + '}'.join(lista_prompts_aluno))\n",
    "f.close()\n",
    "\n",
    "# lê o arquivo e salva todo o texto na variável 'prompts'\n",
    "f = open(\"prompts_aluno.txt\", \"r\")\n",
    "prompts = f.read()\n",
    "f.close()\n",
    "\n",
    "# ao fim da execução do programa, é indicada a evolução do estudante a partir de todos os prompts de todas as sessões\n",
    "print(reconhece_nivel_prompts_aluno(prompts))\n",
    "print('Finalizando o programa')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
